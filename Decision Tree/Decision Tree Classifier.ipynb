{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, feature= None, entropy= 0, clf_metric_val= 0, class_val={}, class_=\"\"):\n",
    "        self.feature= feature   # Feature name to split upon\n",
    "        self.entropy= entropy   # current entropy\n",
    "        self.clf_metric_val= clf_metric_val   # value of classfication metric used\n",
    "        self.children= {}       # dict of child nodes with key as feature value and value as Node object \n",
    "        self.class_val= class_val  #dictionary where key is class name and value is number of data points\n",
    "        self.class_= class_    #class i.e. output return by this node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self,clf_metric=\"gain_ratio\"):\n",
    "        self.__root= None # root node of decision tree\n",
    "        self.clf_metric= clf_metric  #classification metric used\n",
    "        if self.clf_metric not in [\"gain_ratio\", \"gini_index\"]:\n",
    "            self.clf_metric= \"gain_ratio\"\n",
    "        self.__cont_val_features=[]   #index of continuous valued features\n",
    "        self.depth= 0  #depth of decision tree\n",
    "        self.leaf_count=0\n",
    "    \n",
    "    #Private Methods\n",
    "    def __decisionTree(self, X, Y, visited):\n",
    "        visited=list(visited)\n",
    "        y= list(set(Y))\n",
    "        if len(y)==0:\n",
    "            #if a feature that consist of only one value, then splitting on this feature Y will be empty \n",
    "            # None is returned for these features\n",
    "            return None\n",
    "        if len(y)==1:# pure node\n",
    "            newNode= Node(class_val={y[0]: len(Y)}, class_= y[0])\n",
    "            return newNode\n",
    "        \n",
    "        elif visited.count(False)==0: #if no feature is left\n",
    "            entropy, class_val= self.__entrpyANDclass_val(Y)\n",
    "            class_= sorted(class_val, key= lambda x: class_val[x], reverse=True)[0]\n",
    "            newNode= Node(entropy= entropy, class_val= class_val, class_= class_)\n",
    "            return newNode\n",
    "        \n",
    "        from math import inf\n",
    "        \n",
    "        best_to_split= -1   #index of feature with maximum classification metric value\n",
    "        best_metric_val=-1*inf  # maximum metric value\n",
    "        #Choosing a feature for splitting\n",
    "        for i in range(X.shape[1]):\n",
    "            if visited[i]:\n",
    "                continue\n",
    "                \n",
    "            #Calculating metric value for a particular feature\n",
    "            if i in self.__cont_val_features: #checking if a feature is continuous valued or not\n",
    "                val, b= self.__handle_cont_val_features(X[:,i], Y)  #val for metric value and b for decision boundary \n",
    "            else:\n",
    "                val, b= self.__handle_discrt_val_features(X[:,i], Y)\n",
    "            if val> best_metric_val:\n",
    "                best_metric_val= val\n",
    "                best_to_split= i\n",
    "                boundary= b\n",
    "    \n",
    "        visited[best_to_split]= True   # marking chosen feature to be visited\n",
    "        children={}\n",
    "        feature_chose= X[:, best_to_split]\n",
    "        if best_to_split in self.__cont_val_features:\n",
    "            for i in range(2):\n",
    "                if i==0:\n",
    "                    new_x= X[feature_chose <= boundary]\n",
    "                    new_y= Y[feature_chose <= boundary]\n",
    "                else:\n",
    "                    new_x= X[feature_chose > boundary]\n",
    "                    new_y= Y[feature_chose > boundary]\n",
    "                #recursive call\n",
    "                if i==0:\n",
    "                    children[boundary]= self.__decisionTree(new_x, new_y, visited)\n",
    "                    if children[boundary] is None:\n",
    "                        del children[boundary]\n",
    "                else:\n",
    "                    children[inf]= self.__decisionTree(new_x, new_y, visited)\n",
    "                    if children[inf] is None:\n",
    "                        del children[inf]\n",
    "            \n",
    "        else:\n",
    "            if self.clf_metric==\"gain_ratio\":\n",
    "                for i in list(set(feature_chose)):\n",
    "                    new_x= X[feature_chose == i]\n",
    "                    new_y= Y[feature_chose == i]\n",
    "                    children[i]= self.__decisionTree(new_x, new_y, visited) #recursive call\n",
    "                    if children[i] is None:\n",
    "                        del children[i]\n",
    "                    \n",
    "            elif self.clf_metric==\"gini_index\":\n",
    "                #Boundary is a list of 2 tuples\n",
    "                #Because gini index leads to binary split \n",
    "                #Therefore, one tuple contains a single discrete value and second tuple contains all other values of feature\n",
    "                for i in boundary: \n",
    "                    new_x=[]\n",
    "                    new_y=[]\n",
    "                    for j in range(len(feature_chose)):\n",
    "                        if feature_chose[j] in i:\n",
    "                            new_x.append(X[j])\n",
    "                            new_y.append(Y[j])\n",
    "                    new_x= np.array(new_x)\n",
    "                    new_y= np.array(new_y)\n",
    "                    children[i]= self.__decisionTree(new_x, new_y, visited) #recursive call\n",
    "                    if children[i] is None:\n",
    "                        del children[i]\n",
    "\n",
    "        entropy, class_val= self.__entrpyANDclass_val(Y)\n",
    "        class_= sorted(class_val, key= lambda x: class_val[x], reverse=True)[0]\n",
    "        newNode= Node(feature= best_to_split, entropy= entropy, clf_metric_val= best_metric_val, class_val= class_val, \n",
    "                      class_= class_)  #creating a new node object\n",
    "        newNode.children= children\n",
    "\n",
    "        return newNode\n",
    "                \n",
    "    def __chk_cont_val_feature(self, X):\n",
    "        #setting a threshold at 10\n",
    "        #if number of distinct values of a feature is greater than 10, then it is a continuous feature\n",
    "        #else a categorical feature\n",
    "        if len(set(X))>10:\n",
    "            return True #continuous valued feature\n",
    "        else:\n",
    "            return False #categorical feature\n",
    "        \n",
    "    def __entrpyANDclass_val(self, Y):\n",
    "        #Calculating Entropy or information required\n",
    "        from collections import Counter\n",
    "        from math import log\n",
    "        class_val= dict(Counter(Y)) #a dictionary with key as class and value as number of data points\n",
    "        entropy= 0\n",
    "        for i in class_val:\n",
    "            entropy+=(-1*class_val[i]/len(Y))*log(class_val[i]/len(Y),2) #calculating entropy =-sigma(plogp)\n",
    "            \n",
    "        return round(entropy,3), class_val\n",
    "    \n",
    "    def __handle_cont_val_features(self, X, Y):\n",
    "        #handling continuous valued features\n",
    "        from math import inf\n",
    "        \n",
    "        x= list(set(X))\n",
    "        x.sort() #sorting distinct values of feature \n",
    "        max_val=-1*inf #maximum metric value\n",
    "        boundary= None #corresponding boundary value\n",
    "        for i in range(1, len(x)):\n",
    "            mid= (x[i]+x[i-1])/2 #calculating mid value\n",
    "            split_new_y=[[],[]]  #splitting Y according to mid value\n",
    "            split_new_y[0]= Y[X <= mid]\n",
    "            split_new_y[1]= Y[X > mid]\n",
    "            if self.clf_metric == \"gain_ratio\":\n",
    "                val= self.__gain_ratio(Y, split_new_y)  #calculating gain ratio\n",
    "            elif self.clf_metric == \"gini_index\":\n",
    "                val= self.__gini_index(Y, split_new_y)  #calculating delta G(f)= Gini_org- Gini_split\n",
    "            \n",
    "            if val> max_val:\n",
    "                max_val= val\n",
    "                boundary= round(mid,5)\n",
    "        \n",
    "        return max_val, boundary\n",
    "    \n",
    "    def __handle_discrt_val_features(self, X, Y):\n",
    "        #handling categorical features\n",
    "        x= list(set(X)) #distinct values of feature X\n",
    "        if self.clf_metric == \"gain_ratio\":\n",
    "            split_y= [ Y[X ==x[i]] for i in range(len(x))] #splitting Y for all distinct values of X\n",
    "            val= self.__gain_ratio(Y, split_y) #calculating gain ratio\n",
    "            return val, None\n",
    "        elif self.clf_metric == \"gini_index\":\n",
    "            from math import inf\n",
    "            \n",
    "            max_val= -1*inf #maximum value of delta gini\n",
    "            boundary= None\n",
    "            #using one vs rest approach\n",
    "            #For eg if a feature= [red, green, blue]\n",
    "            #then splitting Y if feature value is red or non-red(i.e. green or blue)\n",
    "            for i in range(len(x)):\n",
    "                split_y=[[],[]]\n",
    "                split_y[0]= Y[X == x[i]]\n",
    "                split_y[1]= Y[X != x[i]]\n",
    "                val= self.__gini_index(Y, split_y)\n",
    "                if val> max_val:\n",
    "                    max_val= val\n",
    "                    x1= list(x)\n",
    "                    x1.remove(x[i])\n",
    "                    x1= tuple(x1)\n",
    "                    boundary= [(x[i],), x1]  #splitting feature value \n",
    "            return max_val, boundary\n",
    "        \n",
    "    def __gain_ratio(self, Y_org, split_Y):  #list of numpy array of splitted Y is send as an argument\n",
    "        #Calculating gain ratio\n",
    "        from math import log, inf\n",
    "        \n",
    "        info_org, class_val= self.__entrpyANDclass_val(Y_org) #calculating info required for orginal Y\n",
    "        #class_val is of no use\n",
    "        info_f=0  #weighted informaton required for splitted Y\n",
    "        split_info= 0\n",
    "        D= len(Y_org)  #total number of data points in original data\n",
    "        for y in split_Y:\n",
    "            info, class_val= self.__entrpyANDclass_val(y)  #inforamtion required for splitted Y\n",
    "            info_f+= ((len(y)/D)*info)  #(|D1|/|D|)*info_req(D1)\n",
    "            split_info+= (-1*(len(y)/D)*log(len(y)/D, 2))  #(|D1|/|D|)*log2((|D1|/|D|))\n",
    "            \n",
    "        info_gain= info_org- info_f  #information gain\n",
    "        \n",
    "        if split_info==0:\n",
    "            return inf\n",
    "        return round(info_gain/ split_info,3)  #Gain ratio\n",
    "        \n",
    "    def __gini_index(self, Y_org, split_Y):\n",
    "        #Gini_index= 1-sigma(p**2)\n",
    "        from collections import Counter\n",
    "        \n",
    "        class_val= dict(Counter(Y_org))\n",
    "        D= len(Y_org)\n",
    "        gini_org=1\n",
    "        gini_split=0\n",
    "        \n",
    "        for i in class_val:\n",
    "            gini_org-= (class_val[i]/D)**2 #Calculating gini index of original Y\n",
    "        for y in split_Y:\n",
    "            gini=1\n",
    "            cls_v= dict(Counter(y))\n",
    "            for i in cls_v:\n",
    "                gini-= (cls_v[i]/len(y))**2 #Calculating gini index of splitted Y\n",
    "            gini_split+= ( (len(y)/D)*gini ) #weighted adding of each split\n",
    "        \n",
    "        return round(gini_org-gini_split,3)  #return difference in gini\n",
    "    \n",
    "    \n",
    "    def __print_tree_helper(self, node, level, col_name):\n",
    "        #printing each node of a tree\n",
    "        if node is None:\n",
    "            return\n",
    "        if len(node.children)==0:\n",
    "            #printing leaf nodes\n",
    "            self.leaf_count+=1#updating value of leaf count\n",
    "            if self.depth<=level:\n",
    "                self.depth= level+1#updating depth\n",
    "            print(f\"Level {level}\") #printing using f-strings\n",
    "            for i in sorted(node.class_val):\n",
    "                print(f\"Count of {i}= {node.class_val[i]}\")\n",
    "            print(f\"Current Entropy is= {node.entropy}\")\n",
    "            print(\"Reached leaf node\\n\")\n",
    "            \n",
    "            return\n",
    "        \n",
    "        #printing non-leaf nodes\n",
    "        print(f\"Level {level}\")\n",
    "        for i in sorted(node.class_val):\n",
    "            print(f\"Count of {i}= {node.class_val[i]}\")\n",
    "        print(f\"Current Entropy is= {node.entropy}\")\n",
    "        if col_name==[]:\n",
    "            print(f\"Splitting on feature X{node.feature} with {self.clf_metric} {node.clf_metric_val}\\n\")\n",
    "        else:\n",
    "            print(f\"Splitting on feature {col_name[node.feature]} with {self.clf_metric} {node.clf_metric_val}\\n\")\n",
    "        \n",
    "        for i in node.children:\n",
    "            self.__print_tree_helper(node.children[i], level+1, col_name) #recursive call for each children\n",
    "       \n",
    "    #Public Methods\n",
    "    def print_tree(self, col_name=[]):\n",
    "        #printing tree nodes with features names as given in col_names\n",
    "        self.__print_tree_helper(self.__root, 0, col_name)\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        #Trainnig decision tree\n",
    "        visited= [False for i in range(X.shape[1])] #visited list of features\n",
    "        for i in range(X.shape[1]):#storing index of each continuous feature\n",
    "            if self.__chk_cont_val_feature(X[:,i]):\n",
    "                self.__cont_val_features.append(i)\n",
    "        self.__root= self.__decisionTree(X, Y, visited)\n",
    "        \n",
    "    def predict_val(self, x, node):\n",
    "        #predicting value for 1 datapoint\n",
    "        if node is None:\n",
    "            return np.nan\n",
    "        if len(node.children)==0: #leaf node\n",
    "            return node.class_\n",
    "        try:\n",
    "            if node.feature in self.__cont_val_features: #for continuous features\n",
    "                boundary_val= sorted(node.children)\n",
    "                if x[node.feature]<=boundary_val[0]:\n",
    "                    return self.predict_val(x, node.children[boundary_val[0]])\n",
    "                else:\n",
    "                    return self.predict_val(x, node.children[boundary_val[1]])\n",
    "            else: #for discrete features\n",
    "                if self.clf_metric=='gain_ratio': \n",
    "                    if x[node.feature] in node.children: #if feature value is any of the key of children of this node\n",
    "                        return self.predict_val(x, node.children[x[node.feature]])\n",
    "                    else: #else return output of this node\n",
    "                        return node.class_\n",
    "                elif self.clf_metric=='gini_index':\n",
    "                    boundary_val= sorted(node.children)\n",
    "                    if x[node.feature] in boundary_val[0]:#if feature value is in any of the key of children of this node\n",
    "                        return self.predict_val(x, node.children[boundary_val[0]])\n",
    "                    elif x[node.feature] in boundary_val[1]:\n",
    "                        return self.predict_val(x, node.children[boundary_val[1]])\n",
    "        except:\n",
    "            pass\n",
    "        return node.class_\n",
    "    \n",
    "    def predict(self, X):\n",
    "        #predicting value by traversing tree for each data-point\n",
    "        Y_pred= []\n",
    "        for i in range(X.shape[0]):\n",
    "            Y_pred.append(self.predict_val(X[i], self.__root))\n",
    "        Y_pred= np.array(Y_pred)\n",
    "        return Y_pred\n",
    "    \n",
    "    def score(self, Y_truth, Y_pred):\n",
    "        #Calculating precentage accuracy\n",
    "        accurate=0\n",
    "        for i in range(len(Y_truth)):\n",
    "            if(Y_truth[i]== Y_pred[i] ):\n",
    "                accurate+=1\n",
    "        return accurate/len(Y_truth)\n",
    "    \n",
    "    def export_to_pdf(self, feature_names=None, class_names=None, filename=None):\n",
    "        #Representing formed decision tree in graphical form\n",
    "        #pip install graphviz\n",
    "        from graphviz import Digraph\n",
    "        dot= Digraph()\n",
    "        dot.attr('node',shape=\"rectangle\") #changing shape of each node to rectangle\n",
    "        parent=[] #a Queue to save parent node of each node so that edges can be created\n",
    "        queue= [] #Queue to store current nodes\n",
    "        queue.append(self.__root)\n",
    "        j=0\n",
    "        while len(queue)!=0:  #Level order Traversal\n",
    "            nodecount= len(queue)\n",
    "            while nodecount:\n",
    "                nodecount-=1\n",
    "                node= queue.pop(0)\n",
    "                if len(node.children)!=0:  #if not a leaf node, include feature in the label of node\n",
    "                    if feature_names is not None:\n",
    "                        label= f\"{feature_names[node.feature]}\"\n",
    "                        if node.feature in self.__cont_val_features:\n",
    "                            label+= f\"<={sorted(node.children)[0]}\"\n",
    "                        label+=\"\\n\"\n",
    "                    else:\n",
    "                        label= f\"X{node.feature}\"\n",
    "                        if node.feature in self.__cont_val_features:\n",
    "                            label+= f\"<={sorted(node.children)[0]}\"\n",
    "                        label+=\"\\n\"\n",
    "                else:\n",
    "                    label=\"\"\n",
    "                #Values stored in each node of tree\n",
    "                #here value of gini index is delta gini(f)\n",
    "                label+=f\"{self.clf_metric}= {node.clf_metric_val}\\nEntropy= {node.entropy}\\nvalues= {node.class_val}\\n\"\n",
    "                if class_names is not None: #class returned by a node\n",
    "                    label+=f\"class= {class_names[node.class_]}\\n\"\n",
    "                dot.node(str(j), label) #creating a node with identifier j and value of node is label\n",
    "                if len(parent)!=0:  #not root node\n",
    "                    prnt=parent.pop(0)\n",
    "                    #creating edges with label on each edge like true, false and discrete values of a feature\n",
    "                    dot.edge(prnt[0], str(j),label= prnt[1]) \n",
    "                \n",
    "                fl= True #first children of continuous valued feature node is true and second is false\n",
    "                #to identify first and second children fl is used\n",
    "                for i in sorted(node.children):\n",
    "                    if node.feature in self.__cont_val_features:\n",
    "                        if fl:\n",
    "                            label=\"True\" #edge label\n",
    "                            fl=False\n",
    "                        else:\n",
    "                            label=\"False\" #edge label\n",
    "                    else:\n",
    "                        if self.clf_metric==\"gain_ratio\":\n",
    "                            label=str(i) #edge label-> discrete values\n",
    "                        elif self.clf_metric==\"gini_index\":\n",
    "                            if fl:\n",
    "                                label= str(i[0]) #edge label\n",
    "                                fl= False\n",
    "                            else:\n",
    "                                label= \"rest\" #edge label\n",
    "                    parent.append((str(j), label)) #storing identifier of parent node\n",
    "                    queue.append(node.children[i]) #storing children of current node\n",
    "                j+=1\n",
    "        if filename is not None:\n",
    "            import pydotplus\n",
    "            graph = pydotplus.graph_from_dot_data(dot.source)\n",
    "            graph.write_pdf(filename) # creating pdf using dot data\n",
    "        else:\n",
    "            return dot.source\n",
    "    \n",
    "    def confusion_matrix(self, Y_truth, Y_pred):\n",
    "        #calculating confusion matrix\n",
    "        matrix= [[0 for j in range(max(Y_truth)+1)] for i in range(max(Y_truth)+1)]\n",
    "        for i in range(len(Y_truth)):\n",
    "            matrix[Y_truth[i]][Y_pred[i]]+=1\n",
    "        for i in range(len(matrix[0])):\n",
    "            print(matrix[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x=np.array([[0,0],[0,1],[1,0]])\n",
    "y=np.array([0,1,1])\n",
    "clf= DecisionTree(clf_metric= \"gini_index\")\n",
    "clf.fit(x,y)\n",
    "clf.print_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.export_to_pdf(filename=\"OR.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict(np.array([[1,1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.confusion_matrix([1],[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X,Y = datasets.make_classification(n_samples=1000, n_features=5, n_classes=3,n_informative=3 , random_state=0)\n",
    "for i in range(X.shape[0]):\n",
    "    for j in [0,2,4]:\n",
    "        X[i][j]=int(X[i][j])\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=0)\n",
    "clf= DecisionTree(clf_metric=\"gini_index\")\n",
    "clf.fit(X_train,Y_train)\n",
    "clf.print_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.export_to_pdf(filename= \"Random_Dataset.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.leaf_count,clf.depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred= clf.predict(X_test)\n",
    "clf.score(Y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred= clf.predict(X_train)\n",
    "clf.score(Y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris= datasets.load_iris()\n",
    "clf= DecisionTree()\n",
    "x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state = 1)\n",
    "clf.fit(x_train, y_train)\n",
    "clf.print_tree(col_name= iris.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred= clf.predict(x_train)\n",
    "y_test_pred= clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.export_to_pdf(feature_names= iris.feature_names, class_names= iris.target_names, filename=\"iris2.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf.leaf_count, clf.depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.confusion_matrix(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.confusion_matrix(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = DecisionTreeClassifier()\n",
    "clf1.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydotplus\n",
    "from sklearn.tree import export_graphviz\n",
    "dot_data = export_graphviz(clf1, out_file=None,\n",
    "                          feature_names=iris.feature_names,\n",
    "                          class_names=iris.target_names)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "graph.write_pdf(\"iris.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred1 = clf1.predict(x_train)\n",
    "y_test_pred1 = clf1.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1.score(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_train, y_train_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_test_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
