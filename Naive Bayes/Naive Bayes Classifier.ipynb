{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import model_selection\n",
    "from sklearn import datasets\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    def __init__(self):\n",
    "        self.__cont_val_features=[]\n",
    "        self.prob= {}\n",
    "    \n",
    "    def fit(self, X_train, Y_train):\n",
    "        self.prob={}\n",
    "        self.prob[\"total_count\"]=  len(Y_train)\n",
    "        pos_class= set(Y_train)\n",
    "        for cur_class in pos_class:\n",
    "            self.prob[cur_class]={}\n",
    "            X_train_current= X_train[Y_train == cur_class]\n",
    "            Y_train_current= Y_train[Y_train == cur_class]\n",
    "            self.prob[cur_class][\"count\"]= len(Y_train_current)\n",
    "            num_features= X_train.shape[1]\n",
    "            \n",
    "            for j in range(1, num_features+1):\n",
    "                self.prob[cur_class][j]={}\n",
    "                X_j_unique= set(X[:, j-1])\n",
    "                for unique_val in X_j_unique:\n",
    "                    self.prob[cur_class][j][unique_val]= (X_train_current[:,j-1]== unique_val).sum()\n",
    "    \n",
    "    def __predict_one(self, X):\n",
    "        best_p= None\n",
    "        best_class= None\n",
    "        first_run= True\n",
    "        for cur_class in self.prob:\n",
    "            if cur_class== \"total_count\":\n",
    "                continue\n",
    "            # Because prob can be very low and multiply with small numbers leads to more smaller numbers\n",
    "            # Thats why instead of multiplying, we'll be adding small numbers using log\n",
    "            cur_prob= np.log(self.prob[cur_class][\"count\"])- np.log(self.prob[\"total_count\"])\n",
    "            num_features= len(X)\n",
    "            \n",
    "            for j in range(1, num_features+1):\n",
    "                cur_prob+= np.log(self.prob[cur_class][j][X[j-1]]+1)-np.log(self.prob[cur_class][\"count\"] +\n",
    "                                                                            len(self.prob[cur_class][j]))\n",
    "            \n",
    "            if first_run or cur_prob> best_p:\n",
    "                best_p= cur_prob\n",
    "                best_class= cur_class\n",
    "            first_run = False\n",
    "            \n",
    "        return best_class\n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        y_pred= []\n",
    "        for x in X_test:\n",
    "            y_pred_one= self.__predict_one(x)\n",
    "            y_pred.append(y_pred_one)\n",
    "        \n",
    "        return np.array(y_pred)\n",
    "    \n",
    "    def score(self, Y_truth, Y_pred):\n",
    "        from collections import Counter\n",
    "        dict_Y_truth= dict(Counter(Y_truth))\n",
    "        dict_Y_pred= dict(Counter(Y_pred))\n",
    "        accurate=0\n",
    "        for i in dict_Y_truth:\n",
    "            if i in dict_Y_pred:\n",
    "                accurate+=min(dict_Y_truth[i],dict_Y_pred[i])\n",
    "        coeff= accurate/len(Y_truth)\n",
    "        return coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeLabelled(column):\n",
    "    second_limit = column.mean()\n",
    "    first_limit = 0.5 * second_limit\n",
    "    third_limit = 1.5*second_limit\n",
    "    for i in range (0,len(column)):\n",
    "        if (column[i] < first_limit):\n",
    "            column[i] = 0\n",
    "        elif (column[i] < second_limit):\n",
    "            column[i] = 1\n",
    "        elif(column[i] < third_limit):\n",
    "            column[i] = 2\n",
    "        else:\n",
    "            column[i] = 3\n",
    "    return column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "Y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,X.shape[-1]):\n",
    "    X[:,i] = makeLabelled(X[:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1= NaiveBayes()\n",
    "clf1.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1.prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_pred= clf1.predict(X_test)\n",
    "print(\"Test Score: \", clf1.score(Y_test, Y_test_pred))\n",
    "\n",
    "Y_train_pred= clf1.predict(X_train)\n",
    "print(\"Train Score: \", clf1.score(Y_train, Y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(Y_test,Y_test_pred))\n",
    "print(confusion_matrix(Y_test,Y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = GaussianNB()\n",
    "clf2.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_pred_skl = clf2.predict(X_test)\n",
    "print(\"Test Score: \", clf2.score(X_test, Y_test))\n",
    "\n",
    "Y_train_pred_skl= clf2.predict(X_train)\n",
    "print(\"Train Score: \", clf2.score(X_train, Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(Y_test,Y_test_pred_skl))\n",
    "print(confusion_matrix(Y_test,Y_test_pred_skl))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
